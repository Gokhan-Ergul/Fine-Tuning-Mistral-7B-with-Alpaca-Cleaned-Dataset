# Fine-Tuning-Mistral-7B-with-Alpaca-Cleaned-Dataset

This project demonstrates a lightweight fine-tuning experiment using the  
`mistralai/Mistral-7B-v0.1` model and the `yahma/alpaca-cleaned` dataset.  
To keep training fast and resource-friendly, the dataset and hyperparameters were intentionally minimized.

## ✅ Project Overview

- **Base Model:** `mistralai/Mistral-7B-v0.1`  
- **Dataset:** `yahma/alpaca-cleaned`
- **Subset Used:** 2,000 training samples  
- **Quantization:** 4-bit  
- **Fine-Tuning Method:** LoRA  
- **Epochs:** 1 (for quick experimentation)  
- **Goal:** Provide a minimal, reproducible notebook for rapid prototyping.

## ✅ Dataset Preparation

To speed up training, only a subset of the training split was used:

```python
mini_ds = ds["train"].shuffle(seed=42).select(range(2000))

